{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Naive Bayes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_texts():\n",
    "    texts = [\n",
    "        (\"Chinese Beijing Chinese\", \"c\"),\n",
    "        (\"Chinese Chinese Shanghai\", \"c\"),\n",
    "        (\"Chinese Macao\", \"c\"),\n",
    "        (\"Tokyo Japan Chinese\", \"j\"),\n",
    "    ]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def predict_class(d: str, training_texts: list):\n",
    "    \n",
    "    s1_freqs = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    \n",
    "    vocab = set()\n",
    "    for text, label in training_texts:  # go through all training samples\n",
    "        for word in text.split():   # tokenize training document/text\n",
    "            vocab.add(word)  # building vocabulary\n",
    "            s1_freqs[label][word] += 1  # maintain frequency\n",
    "    \n",
    "    # Compute vocabulary Size\n",
    "    V = len(vocab)\n",
    "\n",
    "    # calculate class priors\n",
    "    class_wise_den = defaultdict(lambda: 0)\n",
    "\n",
    "    for c, word_pair in s1_freqs.items():\n",
    "        class_wise_den[c] += sum(word_pair.values())\n",
    "\n",
    "    # Computing predictor priors     \n",
    "    _, labels = list(zip(*training_texts))\n",
    "    all_classes = np.unique(labels)\n",
    "    priors = {c: freq / len(labels) for c, freq in Counter(labels).items()}\n",
    "\n",
    "    # Compute likelihoods\n",
    "    likelihood = defaultdict(lambda: defaultdict(lambda : int))\n",
    "    \n",
    "    for c in priors:\n",
    "        for word in vocab:\n",
    "            prob = (s1_freqs[c][word] + 1.) / (class_wise_den[c] + V) \n",
    "            likelihood[c][word] = prob\n",
    "    \n",
    "    # Computing posteriors probabilities for all classes    \n",
    "    final_probs = defaultdict(lambda: defaultdict(lambda: 1))\n",
    "    for c in s1_freqs:\n",
    "        p = priors[c]\n",
    "        for w in d.split():\n",
    "            p = p * likelihood[c][w]\n",
    "        final_probs[c] = p\n",
    "\n",
    "    # Filtering out the class with maxium probability     \n",
    "    max_class, max_prob = '', float('-inf')\n",
    "    for c, prob in final_probs.items():\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            max_class = c\n",
    "        \n",
    "    return max_class, s1_freqs, final_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[6;30;42mSuccess!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "s1_student = predict_class(\"Chinese Chinese Chinese Tokyo Japan\", get_training_texts())\n",
    "assert_equal(len(s1_student), 3, msg=\"Your function does not return 3 values. Check the description again for the requirements that your function must satisfy!\")\n",
    "\n",
    "s1_class, s1_freqs, s1_probs = s1_student\n",
    "assert_equal(s1_class, \"c\", msg=\"Your first return value should be the predicted class. Your function did not classify correctly.\")\n",
    "assert_equal(s1_freqs[\"c\"][\"Chinese\"], 5, msg=\"Your second return value should map class labels to dictionaries containing the frequency of terms in documents of that class in the training set. For example, 'Chinese' appears 5 times in documents with the 'c' class. Your return value does not have that information.\")\n",
    "assert_equal(s1_freqs[\"c\"][\"Tokyo\"], 0, msg=\"Your second return value should map class labels to dictionaries containing the frequency of terms in documents of that class in the training set. For example, 'Tokyo' appears 0 times in documents with the 'c' class. Your return value does not have that information. Use a defaultdict to return 0 when the term does not appear in the dictionary!\")\n",
    "assert_equal(s1_probs[\"c\"],  0.00030121377997263036, msg=\"The class probability for the document given class 'c' is not correct. Did you round the values? This test case works only with unrounded values.\")\n",
    "assert_equal(s1_probs[\"j\"],  0.00013548070246744226, msg=\"The class probability for the document given class 'j' is not correct. Did you round the values? This test case works only with unrounded values.\")\n",
    "print(\"\\x1b[6;30;42mSuccess!\\x1b[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[6;30;42mSuccess!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from nose.tools import assert_equal\n",
    "# These tests check that your solution works not only for this one example, but other examples as well\n",
    "\n",
    "texts = [\n",
    "    (\"yellow long long yellow fruit tree\", \"Fruit\"),\n",
    "    (\"yellow yellow mollusk yellow mollusk fruit\", \"Animal\"),\n",
    "    (\"brown fur brown brown forest brown fruit\", \"Animal\"),\n",
    "    (\"red red round fruit fruit forest forest red\", \"Fruit\"),\n",
    "    (\"brown brown fur fur brown fur fur tree\", \"Fruit\"),\n",
    "    (\"red red red yellow yellow yellow red red yellow red\", \"Animal\"),\n",
    "    (\"green green green green green green yellow yellow yellow red red red red red\", \"Color\")\n",
    "]\n",
    "\n",
    "s1_class, s1_freqs, s1_probs = s1_student = predict_class(\"fruit fruit fruit fruit\", texts)\n",
    "assert_equal(s1_class, \"Fruit\", msg=\"Your first return value should be the predicted class. Your function did not classify correctly.\")\n",
    "assert_equal(s1_freqs[\"Fruit\"][\"red\"], 3, msg=\"Your second return value should map class labels to dictionaries containing the frequency of terms in documents of that class in the training set.\")\n",
    "assert_equal(s1_freqs[\"Fruit\"][\"Cthulhu\"], 0, msg=\"Your second return value should map class labels to dictionaries containing the frequency of terms in documents of that class in the training set.\")\n",
    "assert_equal(s1_freqs[\"Animal\"][\"yellow\"], 7, msg=\"Your second return value should map class labels to dictionaries containing the frequency of terms in documents of that class in the training set.\")\n",
    "assert_equal(s1_freqs[\"Color\"][\"green\"], 6, msg=\"Your second return value should map class labels to dictionaries containing the frequency of terms in documents of that class in the training set.\")\n",
    "assert_equal(s1_probs[\"Fruit\"],  9.251399183780853e-05, msg=\"The class probability for the document given class 'Fruit' is not correct. Did you round the values? This test case works only with unrounded values.\")\n",
    "assert_equal(s1_probs[\"Animal\"],  2.5977213600685546e-05, msg=\"The class probability for the document given class 'Animal' is not correct. Did you round the values? This test case works only with unrounded values.\")\n",
    "assert_equal(s1_probs[\"Color\"],  3.657142857142857e-07, msg=\"The class probability for the document given class 'Color' is not correct. Did you round the values? This test case works only with unrounded values.\")\n",
    "\n",
    "# second example\n",
    "s1_class, s1_freqs, s1_probs = s1_student = predict_class(\"green green round fruit\", texts)\n",
    "assert_equal(s1_class, \"Color\", msg=\"Your first return value should be the predicted class. Your function did not classify correctly.\")\n",
    "print(\"\\x1b[6;30;42mSuccess!\\x1b[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> KNN </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "def get_flower_data():\n",
    "    iris = load_iris(as_frame=True)\n",
    "    df = iris[\"data\"]\n",
    "    df[[\"sepal length (cm)\", \"sepal width (cm)\"]].to_records(index=False)\n",
    "\n",
    "    nodes = [(length, width, x) for x, (length, width) in zip(iris.target, df[[\"sepal length (cm)\", \"sepal width (cm)\"]].to_records(index=False))]\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def k_nearest_neighbors(node, nodes, k=3):\n",
    "    \n",
    "    x1, x2, y = list(zip(*nodes))\n",
    "    X = list(zip(x1, x2))\n",
    "    X = np.array(X)\n",
    "    node = np.array([node])\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, p=2,  weights='uniform')\n",
    "    knn.fit(X, y)\n",
    "    pred = knn.predict(node)\n",
    "    return pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can experiment with your solution here\n",
    "nodes = get_flower_data()\n",
    "# example of how your function will be called\n",
    "# k_nearest_neighbors((7, 2), nodes, 10)\n",
    "# nodes\n",
    "k_nearest_neighbors((7, 2), nodes, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[6;30;42mSuccess!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "s4_nodes = get_flower_data()\n",
    "# assert_equal(k_nearest_neighbors((7, 2), nodes, 10), 2)  # Exceptionally wrong input \n",
    "assert_equal(k_nearest_neighbors((7, 2), nodes, 3), 1)\n",
    "assert_equal(k_nearest_neighbors((7, 2), nodes, 5), 1)\n",
    "assert_equal(k_nearest_neighbors((7, 2), nodes, 7), 1)\n",
    "assert_equal(k_nearest_neighbors((7, 2), nodes, 9), 2)\n",
    "assert_equal(k_nearest_neighbors((7, 2), nodes, 11), 2)\n",
    "assert_equal(k_nearest_neighbors((6, 2.8), nodes, 9), 1)\n",
    "assert_equal(k_nearest_neighbors((-6, 2.8), nodes, 9), 0)\n",
    "assert_equal(k_nearest_neighbors((4.77, 3.99), nodes, 9), 0)\n",
    "assert_equal(k_nearest_neighbors((4.77, 3.99), nodes, 3), 0)\n",
    "assert_equal(k_nearest_neighbors((5.77, 2.8), nodes, 121), 1)\n",
    "print(\"\\x1b[6;30;42mSuccess!\\x1b[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Evaluation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_pred, y_true, labels):\n",
    "    precision, recall, f1 = {}, {}, {}\n",
    "    class_report = classification_report(y_true, y_pred, labels=labels, output_dict=True)\n",
    "    # Structuring the output    \n",
    "    for l in labels:\n",
    "        precision[l] = class_report[str(l)]['precision']\n",
    "        recall[l] = class_report[str(l)]['recall']\n",
    "        f1[l] = class_report[str(l)]['f1-score']\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[6;30;42mSuccess!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "s3_solution = evaluate([1, 2, 3, 1, 2, 3], [1, 2, 1, 2, 1, 3], [1, 2, 3])  # The input was modified for consistency\n",
    "assert_equal(len(s3_solution), 3, msg=\"Your function should return three values.\")\n",
    "precision, recall, f1 = s3_solution\n",
    "assert_equal(precision[1], 0.5, msg=\"Your precision for class 1 is wrong.\")\n",
    "assert_equal(precision[2], 0.5, msg=\"Your precision for class 2 is wrong.\")\n",
    "assert_equal(precision[3], 0.5, msg=\"Your precision for class 3 is wrong.\")\n",
    "assert_equal(recall[1], 0.3333333333333333, msg=\"Your recall for class 1 is wrong.\")\n",
    "assert_equal(recall[2], 0.5, msg=\"Your recall for class 2 is wrong.\")\n",
    "assert_equal(recall[3], 1, msg=\"Your recall for class 3 is wrong.\")\n",
    "assert_equal(f1[1], 0.4, msg=\"Your f1 for class 1 is wrong.\")\n",
    "assert_equal(f1[2], 0.5, msg=\"Your f1 for class 2 is wrong.\")\n",
    "assert_equal(f1[3], 0.6666666666666666, msg=\"Your f1 for class 3 is wrong.\")\n",
    "print(\"\\x1b[6;30;42mSuccess!\\x1b[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Task 2G</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_solution = \"A)\"  # Could possibly be a 'D)' as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Task 4G</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "s5_solution = \"D)\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tf_1_14",
   "language": "python",
   "name": "venv_tf_1_14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
